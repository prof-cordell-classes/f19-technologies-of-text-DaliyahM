### Lab Report: _Corpus Analysis_ 

#### Daliyah Middleton

November 13, 2019

___

**1). Process Description**:   
   
For this lab, we worked on the online rendition of the computer program, _R Studio_. We began by using the _Text as Data_ file, to pull up the instructions that we would cycle through for this lab. As a short introduction to computing with R studio, we went through several exercises which included running code, how to write our own code, defining a variable, and running multiple packages in order to create different representations of data forms (tables, charts, graphs). For this lab in particular, we used texts from Project Gutenberg (specifically, _Scenes of Clerical Life_, and Jane Austen's work), and we titled our variable ‘Barton’. We also looked closer at the text itself, and we learned how to run code that would shorten the Gutenberg paratext, as well as the ‘\r’ and ‘\n’ placeholders for punctuation marks. When we started looking at the text as data, we first filtered through the words in the text, which created a table that listed all the words in the text and its frequency throughout the book. While this data may be useful for particular findings, for this specific lab, we wanted to look further into more detailed pieces of data that would tell us more about the actual literature.   
 
From this, we ran another code that eliminated the linguistic “token” words, to look for words with more semantic quality. After running this code, another table was created that also showed the frequency of words in the text, from this table we could decipher that the most used words were “miss”, “sir”, and “Caterina”. We were also able to run packages that computed the word frequency along with the ratio of the words appearance in the texts (with the mutate feature).Moreover, we were able to choose our own text from Project Gutenberg to run the codes with. I used the _Peter Pan and Wendy_ text with the variable ‘peter’. With the _Peter and Wendy_ text I also ran similar packages that looked at the frequency of the words as well as their ratios. As an introduction to our upcoming lab on corpus analysis, this exploration of R Studio allowed us to grasp an understanding of the basis of the computer program before we officially put it to work through our own creative data endeavors.  
    
**2). Observations**:   

Initially, as I have worked with R studio in the past to formulate data for a textual corpus (in the _Literature and Digital Diversity Course_ with Professor Dillon), I presumed that this activity will be of similar nature. As I had not used R studio since I’ve taken that class, the introduction on the basic tools in R and how to use the different areas including the console, plots, and environment was a good refresher on how to utilize R for data analysis. In particular, I really appreciated the detailed descriptions of the different terms and symbols used in R to show how the system is computing the data. For instance, learning that “tokens” connects to linguistics and that it represents a unit of language, helped me conceptualize the work that the computer is performing in order to create the data sets. I think that this ties into Ben Schmidt’s question in regards to whether or not humanist need to understand algorithms, in which I do agree that humanist should understand the language for literacy purposes. I think that a lot of the reason why I’m not as interested in computing or engaging in any code websites is primarily because I don’t necessarily understand the code and how it is being processed. I think that if I understood explicitly what was occurring in the code and how the computer is making specific decisions on how to demonstrate the algorithm I wouldn’t be uneasy about writing my own code and thinking of different ways to change and interpret data.   

Furthermore, when I went back into R studio to look over my work, I noticed that there were specific packages that you have to run in order to run subsequent packages. For instance, I noticed that there were ‘Error’ messages when I tried to run the code to create the ‘barton’ variable to view the table with the word frequency, before I had run the "view barton" package and the Project Gutenberg text. I think that knowing that you have to follow a specific order before you run every package proves that there not only is a structure to computing code but also a premeditated way to follow the specific structure. For example, when I ran my example book from Project Gutenberg, _Peter and Wendy_, I had to duplicate the same format as the ‘barton’ variable to produce similar packages, except with a new variable under ‘peter’. Ultimately, the structure and examining the literal code helped me feel more comfortable with exploring the different data sets and understanding the computation behind the system. 
   

**3). Analysis**:    
    
In Professor Dillon’s course, I remember creating a textual corpus of children’s literature with over 800,000 words. For this project we had to develop our own research question and utilize the data in the corpus to answer our initial question. From this, I proposed to explore the relevance of magic and fantasy in children’s literature and examine what these themes are teaching children. More specifically, I looked through different vectors of magic (dark and light) and power, its influence on children, along  with a connection to Shakespeare’s _The Tempest_ (for the assignment). As I had to go through several data sets, and compute different clusters to analyze words that related to magic and power, I was able to specify the terms that I wanted to use to make the final claim in my paper. Overall, when considering this project from my previous course, and looking into the continuation of this lab, I think this quote regarding the use of data is important to recognize:   
 
>”you always ask questions about the categories that structure your data, and the systems of power that might, in turn, have structured them”(_What Gets Counted Counts_, 2018).   
   
I think that it’s imperative that you are actively questioning the different variables that may or may not be represented in the data that is shown in front of you. In my case, as I was working with over 22 children’s books, I had to recognize that my data set would not account for the data represented when looking at all children’s books. I think that while having a smaller data set may be easier to look at and contextualize, a smaller corpora may also mean that you are limiting your data and may not be taking account of every potential aspect of the information. Although I did not have access to every child's story ever made for this assignment (nor would it would’ve been necessary in effectively completing the assignment), I think that recognizing that the data set is limited by some parameters, and recognizing that not everything is represented through my tables is an important step when you are analyzing and computing data.
Moreover, going back to Ben Schmidt’s comment regarding humanists and algorithms makes me think about how these two disciplines may be perceived as different versions of one another. For instance, in _The Thrilling Adventures of Lovelace and Babbage_ , the author, Sydney Padua makes a note in the footnotes that describes the process of turning literature into code:   

>”Data can be read ‘destructively’, meaning the act of reading destroys the original, or nondestructively, where a copy is made leaving the original intact… Book scanning, if automated, must be done destructively, as the pages have to be cut apart to feed through the machine” (Padua 191).    
 
Following this page, Ada Lovelace notes the transformation of the words in the manuscript through the comic bubble “They have become… DATA!” (Padua 192). I think that the author’s interpretation of literature turned into data as a destructive mechanism is an interesting mechanism because it does recognize that the computational process of extracting data from books requires the book to be “cut apart”. For instance, in order for us to analyze the ratios and frequencies of terms in the Jane Austen texts that we examined in class, the book is no longer in its original format when it is going through the system to create the various data tables. Overall, I think that it was an exceptional discovery for Charles Babbage to even think of the Difference and Analytical Machines, with of course the help of Ada Lovelace, which lead to the creation of systems that we use today (such as computers) to complete similar works of data analysis.   
    
---
**References**:
“Project Gutenberg.” Project Gutenberg, https://www.gutenberg.org/.  
D'Ignazio, Catherine, Klein, Lauren. "Data Feminism". MIT Press, 2019. 
Padua, Sydney. _The Thrilling Adventures of Lovelace and Babbage_. Pantheon Books, 2015. 